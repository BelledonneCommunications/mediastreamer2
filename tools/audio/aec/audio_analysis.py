import re
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import librosa
from aec_metrics import AecMetrics

import matplotlib.pyplot as plt


class AudioAnalysis:
    """
    This class handle the analysis of audio data generated by AEC filtering.
    """

    def __init__(self, nearend, aec_output):
        """
        Initialization of the class.
        :param nearend: AudioSignal instance for near-end.
        :param aec_output: AudioSignal instance for output of AEC filter.
        """

        self.nearend = nearend
        self.aec_output = aec_output
        self.log_file = ""
        self.aec_metrics = AecMetrics()
        self.sampling_rate_hz = 0
        self.real_delay_ms = 0
        self.estimated_delay_ms = 0
        self.energy_in_silence = 0
        self.similarity_in_speech = 0
        self.msticker_late_ms = []
        self.maxpos = 0
        self.test_passed = 1
        self.total_time_s = 0
        self.silence_mask = None
        self.start_analysis_time_ms = 0
        self.start_sample = 0
        self.distance_mfcc = 0
        self.similarity_mfcc = 0
        self.filter_stats = None

    def get_results(self, log_file):
        """
        Extracts the results of the AEC filtering during a test from mediastreamer2 test suite AEC3.
        :param log_file: full file name of the log file.
        """

        self.log_file = log_file
        metrics_lines = []
        float_pattern = r'\d+\.\d+|\d+'
        with open(log_file) as f:
            contents = f.readlines()
            for i, line in enumerate(contents):

                if "Initializing WebRTC echo canceler" in line:
                    self.sampling_rate_hz = int(line.split("with sample rate = ")[-1].split(" Hz")[0])
                    print(f"sampling rate found is {self.sampling_rate_hz} Hz")

                if "current metrics :" in line:
                    metrics_lines.append(line)

                if "estimated delay is" in line:
                    self.estimated_delay_ms = int(line.split("estimated delay is ")[-1].split(", real")[0])
                    self.real_delay_ms = int(line.split("real delay is ")[-1].split(" ms")[0])

                if "chunk - max cross-correlation obtained at position " in line and self.maxpos == 0:
                    match = re.search(r'\[(-?\d+)\]',
                                      line.split("obtained at position ")[-1].split(", similarity factor")[0])
                    if match:
                        self.maxpos = int(match.group(1))

                if "Max cross-correlation obtained on speech parts" in line:
                    match = re.search(r'\[([-+]?\d*\.?\d+)\]', line.split("similarity factor=")[-1])
                    if match:
                        self.similarity_in_speech = float(match.group(1))

                if "Energy measured on silences" in line:
                    match = re.search(r'\[([0-9]+\.[0-9]+)\]', line.split("Energy measured on silences=")[-1])
                    if match:
                        self.energy_in_silence = float(match.group(1))

                if "Tester MSTicker: We are late of" in line:
                    late_str = line.split("Tester MSTicker: We are late of ")[-1].split(" milliseconds.")[0]
                    self.msticker_late_ms.append(int(late_str))

                if "FILTER USAGE STATISTICS" in line:
                    k = i + 4
                    if k < len(contents):
                        stats_line = contents[k]
                        while k < len(contents) and "MSWebRTCAEC" not in stats_line and "=======================" not in stats_line:
                            k = k + 1
                        if "MSWebRTCAEC" in stats_line:
                            self.filter_stats = np.array([float(num) for num in re.findall(float_pattern, stats_line.split("MSWebRTCAEC")[1])])

                if "Suite [AEC3] Test [" in line:
                    # Suite [AEC3] Test [Double talk]
                    if "passed in" in line:
                        self.test_passed = 1
                    elif "failed in" in line:
                        self.test_passed = 0
                    total_time_str = line.split("in ")[-1].split(" secs")[0]
                    match = re.search(r'([0-9]+\.[0-9]+)', total_time_str)
                    if match:
                        self.total_time_s = float(match.group(1))

        if len(metrics_lines) > 0:
            self.aec_metrics.get_metrics_from_logs(metrics_lines)

    def print(self):
        """
        Print the results of the AEC test.
        """

        if len(self.msticker_late_ms) > 0:
            print(f"MSticker late: \t{self.msticker_late_ms} ms")
        if self.aec_metrics.metrics is not None:
            print("final AEC metrics:")
            print(f"\tdelay = {self.aec_metrics.delay_final} ms")
            print(f"\terl = {self.aec_metrics.erl_final}")
            print(f"\terle = {self.aec_metrics.erle_final}")
        print(f"estimated delay: \t\t{self.estimated_delay_ms} ms")
        print(f"real final delay: \t\t{self.real_delay_ms} ms")
        print(
            f"maxpos: \t\t\t\t{self.maxpos} samples\n\t\t\t\t\t\t{self.maxpos * 1000. / self.aec_output.sample_rate_hz:0.0f} ms")

        print(f"energy in silence: \t\t{self.energy_in_silence:0.2f}")
        print(f"similarity in speech: \t{self.similarity_in_speech:0.3f}")
        print(f"           with MFCC: \t{self.similarity_mfcc:0.3f}")

        if self.filter_stats is not None:
            print(f"filter usage stats: \tcount\t\t\t{self.filter_stats[0]:0.0f}")
            print(f"\t\t\t\t\t\ttime\tmin\t\t{self.filter_stats[1]:0.2f} ms")
            print(f"\t\t\t\t\t\t\t\tmean\t{self.filter_stats[2]:0.2f} ms")
            print(f"\t\t\t\t\t\t\t\tmax\t\t{self.filter_stats[3]:0.2f} ms")
            print(f"\t\t\t\t\t\t\t\tstd\t\t{self.filter_stats[4]:0.2f} ms")
            print(f"\t\t\t\t\t\tCPU usage\t\t{self.filter_stats[5]:0.2f} %")

        print(f"test passed: \t\t\t{self.test_passed}")
        print(f"total time: \t\t\t{self.total_time_s:0.1f} s")

    def plot_aec_metrics(self, fig_title):
        """
        Plot the audio and the AEC metrics.
        :param fig_title: title of the figure
        :return: plotly figure
        """

        if self.aec_output.normalized_data is None:
            self.aec_output.read_audio(self.sampling_rate_hz)

        fig = self.aec_metrics.plot_with_audio(self.aec_output.timestamps, self.aec_output.normalized_data, fig_title)
        return fig

    @staticmethod
    def compute_correlation(audio_ref, audio_test):
        """
        Return the maximal cross-correlation and its position (in samples) between two audio.
        :param audio_ref: reference audio data
        :param audio_test: tested audio data
        :return: position of the maximal cross-correlation and maximal cross-correlation
        """

        max_length = max(len(audio_ref), len(audio_test))
        padded_ref = np.pad(np.abs(audio_ref), (0, max_length - len(audio_ref)))
        padded_test = np.pad(np.abs(audio_test), (0, max_length - len(audio_test)))
        correlation = np.correlate(padded_ref, padded_test, mode='full')
        max_pos_samples = np.argmax(correlation) - max_length + 1
        corr = max(correlation)
        if len(audio_ref) > len(audio_test):
            max_pos_samples = -1 * max_pos_samples

        return max_pos_samples, corr

    def align_output_and_nearend(self):
        """
        Align the aec output tested audio with near-end reference audio on the maximal correlation.
        """

        if self.nearend.data is not None and self.aec_output.data is not None:
            max_pos_samples, corr = self.compute_correlation(self.nearend.data[self.start_sample:],
                                                             self.aec_output.data[self.start_sample:])
            sample_duration_s = 1. / self.aec_output.sample_rate_hz
            print(
                f"maximum correlation found at {max_pos_samples} samples {sample_duration_s * max_pos_samples * 1000:1.0f} ms with value {corr:1.1f}")

            if max_pos_samples is not None:
                if max_pos_samples > 0:
                    self.nearend.aligned_data = self.nearend.data[self.start_sample:]
                    self.aec_output.aligned_data = self.aec_output.data[self.start_sample + max_pos_samples:]
                elif max_pos_samples < 0:
                    self.nearend.aligned_data = self.nearend.data[self.start_sample + max_pos_samples:]
                    self.aec_output.aligned_data = self.aec_output.data[self.start_sample:]
            min_len = min(len(self.nearend.aligned_data), len(self.aec_output.aligned_data))
            if len(self.aec_output.aligned_data) > min_len:
                self.aec_output.aligned_data = self.aec_output.aligned_data[:min_len]
            elif len(self.nearend.aligned_data) > min_len:
                self.nearend.aligned_data = self.nearend.aligned_data[:min_len]

    def get_silence_mask(self, audio_normalized_data):
        """
        Compute a mask of the silent frames in audio signal (normalized) as a numpy array of the same size with True
         for silent frames, False otherwise.
        :param audio_normalized_data: numpy array, normalized audio signal that is filtered.
        """

        # threshold
        silence_mask = np.zeros_like(audio_normalized_data, dtype=bool)
        if self.aec_output.sample_rate_hz == 48000:
            hw = 240
            th = 0.005
        else:
            hw = 200
            th = 0.001
        for i in range(audio_normalized_data.size):
            w0 = max(0, i - hw)
            wn = min(audio_normalized_data.size, i + hw + 1)
            if np.mean(np.abs(audio_normalized_data[w0:wn])) < th:
                silence_mask[i] = True

        # median filter on mask
        talk_hw = 1400
        silence_tmp = silence_mask.copy()
        for i in range(audio_normalized_data.size):
            w0 = max(0, i - talk_hw)
            wn = min(audio_normalized_data.size, i + talk_hw + 1)
            silence_tmp[i] = np.median(silence_mask[w0:wn])

        self.silence_mask = silence_tmp

    def detect_silence(self):
        """
        Detect the silence in near-end audio and apply the silence mask to generate audio for silence and for talk
        for aec output and near-end.
        """

        if self.nearend.data is None:
            self.silence_mask = np.ones_like(self.aec_output.data[self.start_sample:], dtype=bool)
            self.aec_output.silence = self.aec_output.data[self.start_sample:]
            return

        # detect silence
        self.nearend.normalize_aligned()
        self.get_silence_mask(self.nearend.normalized_aligned_data)

        # non-normalized
        self.aec_output.silence = self.aec_output.aligned_data[self.silence_mask]
        self.nearend.silence = self.nearend.aligned_data[self.silence_mask]
        self.aec_output.talk = self.aec_output.aligned_data[np.logical_not(self.silence_mask)]
        self.nearend.talk = self.nearend.aligned_data[np.logical_not(self.silence_mask)]

    def plot_audio_silence_and_talk(self):
        """
        Plot the compared audio from near-end and aec output, with the silence mask and the related audio for silence
        and talk.
        :return: plotly figure.
        """

        sample_duration_s = 1. / self.aec_output.sample_rate_hz
        colors = ["#636EFA", "#00CC96", "#EF553B", ]
        if self.nearend.aligned_data is not None:
            sig_max = max(np.max(self.nearend.aligned_data), np.max(self.aec_output.aligned_data))
        else:
            sig_max = np.max(self.aec_output.aligned_data)
        fig_title = f'Silence and talk to compare'
        signal_type = ["aligned", "silence", "talk"]
        audio_to_plot = {"near-end": self.nearend,
                         "aec output": self.aec_output}
        n_type = len(signal_type)
        n_audio = len(audio_to_plot.keys())
        fig = make_subplots(rows=n_audio * n_type, cols=1, shared_xaxes=True)

        for i, st in enumerate(signal_type):
            for j, sn in enumerate(audio_to_plot.keys()):

                audio_all = audio_to_plot[sn]
                if sn == "near-end":
                    audio_all = self.nearend
                elif sn == "aec output":
                    audio_all = self.aec_output

                audio = None
                if st == "aligned":
                    audio = audio_all.aligned_data
                elif st == "silence":
                    audio = audio_all.silence
                elif st == "talk":
                    audio = audio_all.talk

                show_legend = True
                if i > 0:
                    show_legend = False

                if audio is not None:
                    signal = audio / sig_max

                    timestamp = sample_duration_s * np.arange(signal.size)
                    fig.add_trace(go.Scatter(x=timestamp, y=signal, mode='lines', line={'width': 1, 'color': colors[j]},
                                             name=f"{sn}", showlegend=show_legend), row=i * n_audio + j + 1, col=1)
                    if st == "aligned":
                        show_mask_legend = True
                        if j > 0:
                            show_mask_legend = False
                        fig.add_trace(
                            go.Scatter(x=timestamp, y=0.7 * self.silence_mask, mode='lines',
                                       line={'width': 2, "color": colors[2]},
                                       name=f"mask", showlegend=show_mask_legend), row=i * n_audio + j + 1, col=1)
                fig.update_yaxes(title_text=f"{st}", range=[-1., 1.], row=i * n_audio + j + 1, col=1)
        fig.update_layout(title=fig_title)
        fig.update_xaxes(title_text='Time')
        fig.show()

        return fig

    def compute_energy_difference_with_nearend(self):
        """
        Compute the energy of the silence parts of near-end and aec output and their difference.
        :return: The energy computed in aec output silence parts or the difference between these energies computed in
        near-end and in aec output.
        """

        if self.nearend.silence is None:
            energy_in_silence = np.sum(np.abs(self.aec_output.silence))
        else:
            energy_in_silence = np.sum(np.abs(self.aec_output.silence)) - np.sum(
                np.abs(self.nearend.silence))
        print(f"energy measured on silence: {energy_in_silence:1.1f}")

        return energy_in_silence

    def compute_acoustic_similarity(self):
        """
        Compute acoustic similarity between two audio signals based on MFCC coefficients.
        See https://librosa.org/doc/0.10.2/generated/librosa.feature.mfcc.html#librosa.feature.mfcc
        :return: matplotlib figure
        """

        if self.sampling_rate_hz <= 16000:
            hop_length = 512
        else:
            hop_length = 1024
        mfcc_ref = librosa.feature.mfcc(y=self.nearend.talk, sr=self.sampling_rate_hz, hop_length=hop_length, htk=True)
        mfcc_test = librosa.feature.mfcc(y=self.aec_output.talk, sr=self.sampling_rate_hz, hop_length=hop_length,
                                         htk=True)

        # compute the mean of the differences for each coefficient
        n_time = mfcc_ref.shape[1]
        print(f"{mfcc_ref.shape[0]} coefs, {n_time} samples")
        print(f"{mfcc_test.shape[0]} coefs, {mfcc_test.shape[1]} samples")
        d = []
        for t in range(n_time):
            d.append(np.linalg.norm(mfcc_ref[:, t] - mfcc_test[:, t]))
        self.distance_mfcc = np.mean(d)

        if self.distance_mfcc == 0:
            self.similarity_mfcc = 1.
        else:
            self.similarity_mfcc = 1. / self.distance_mfcc

        colors = plt.get_cmap("jet")
        x_col = np.linspace(0, 1, mfcc_ref.shape[0])
        diff_lim = [-200, 200]
        n_rows = 4
        fig, ax = plt.subplots(nrows=n_rows, ncols=1, sharex=True, figsize=(20, 15))

        # Set a common color scale for mfcc
        v_min = np.min([mfcc_ref.min(), mfcc_test.min()])
        v_max = np.max([mfcc_ref.max(), mfcc_test.max()])

        librosa.display.specshow(mfcc_ref, ax=ax[0], vmin=v_min, vmax=v_max)
        ax[0].set_title('MFCC, reference')
        ax[0].set_xlabel('Time')
        ax[0].set_ylabel('Frequency range')

        librosa.display.specshow(mfcc_test, ax=ax[1], vmin=v_min, vmax=v_max)
        ax[1].set_title('MFCC, test')
        ax[1].set_xlabel('Time')
        ax[1].set_ylabel('Frequency range')

        librosa.display.specshow(mfcc_ref - mfcc_test, ax=ax[2], vmin=diff_lim[0], vmax=diff_lim[1])
        ax[2].set_title('Diff')
        ax[2].set_xlabel('Time')
        ax[2].set_ylabel('Frequency range')

        plt.subplot(n_rows, 1, 4)
        plt.plot(d, label="instant distance")
        for c in range(mfcc_ref.shape[0]):
            plt.plot(abs(mfcc_ref[c, :] - mfcc_test[c, :]), label=f"distance coef {c}", color=colors(x_col[c]))
        plt.ylim(0, diff_lim[1])

        # Adjust layout and display the plot
        plt.tight_layout()
        # plt.show(block=True)

        return fig

    def compute_echo_cancellation_quality(self, log_file, start_analysis_time_ms=0, plot_silence_and_talk=True):
        """
        Compute or load criteria to measure the quality of the echo cancellation.
        :param log_file: full file name of the log file that contains the traces of the AEC test.
        :param start_analysis_time_ms: time stamp to start the audio analysis in AEC output and near-end audio, in ms.
        Default is 0.
        :param plot_silence_and_talk: True whether the audio must be plotted with silence and talk parts. Default is
        True.
        """

        if start_analysis_time_ms > 0:
            self.start_analysis_time_ms = start_analysis_time_ms
        self.start_sample = int(float(self.start_analysis_time_ms) / 1000.) * self.aec_output.sample_rate_hz
        print(f"\nstart audio comparison at sample {self.start_sample} ({self.start_analysis_time_ms} ms)")

        # load results in log file
        self.get_results(log_file)
        # self.print()

        # align audio if needed
        if self.aec_output.data is not None:
            self.align_output_and_nearend()

            # detect silence and talk
            self.detect_silence()
            if plot_silence_and_talk:
                self.plot_audio_silence_and_talk()

            # compute energy remaining in silence of aec output
            self.compute_energy_difference_with_nearend()

            # compare talk
            if self.nearend.talk is not None:
                fig = self.compute_acoustic_similarity()
                fig.savefig(
                    self.log_file.replace(".log", "_MFCC_comparison_on_talk.png"))
